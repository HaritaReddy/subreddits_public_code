# subreddits_public_code

Contains the code used for generating subreddit rules history (the code is not usable directly since it will need changes in configuration). This is just for reference to show how the processed_rules dataset was generated. Here are the list of code files in the order in which they were executed.

Here, a "cohort" means subreddits created in the same year.

### Initial Input 

The initial input to this workflow is a folder subreddits_by_year/ which contains one json file for each cohort year, named as 2010.json (which contains subreddits created in 2010), 2011.json (which contains subreddits created in 2011) etc. The format of the json file is as follows:

```
<year>.json

{
    <subreddit-name> : (<created_utc>, <subscribers>),
    ...
}
```

The created_utc and subscribers are obtained using PRAW. These values are not needed for further processing. Only the subreddit name (which is the key) is important.

### Step 1

$ python3 snapshot_scraper.py

snapshot_scraper.py: Reads subreddits from the folder subreddits_by_year/ and gets their snapshots using Wayback Machine. The snapshots of the subreddits are written in year-wise json files inside the folder subreddits_snapshots_by_year/. The json files looks as follows:

```
<year>.json

{
    <subreddit-name> : [<snapshot-url-1>, <snapshot-url-2>, ....],
    ...
}
```


### Step 2

$ python3 subreddit_scraper.py

subreddit_scraper.py: Scrapes the rules widget content from the snapshots of each subreddit. It reads the output of the previous step and writes to raw_scraped_rules/ folder. The folder contains subfolders for each year (named as 2010, 2011 etc). Each subfolder contains one json file for a subreddit created in the year (the name of the json file is the name of the subreddit), with the date as key (eg. "20180719") and the value as the string content of the rules widget. An example of how the json file will look like is:

```
{"20180518": "SCRAPING_ERROR", "20181207": [" 1.        no image posts"       2. no hate", "New"], "20181207": [" 1.        no image posts"       2. no hate           3.      no spam", "New"]}
```

The "New" tag indicates that the snapshot from which rules were scraped were in the new format of Reddit rules widget.

### Step 3

$ python3 subreddits_processor.py

subreddits_processor.py: Processes rules in raw_scraped_rules/ by breaking down the content in the rules widget based on the enumeration. This leads to output folder processed_rules/. It contains folders based on the year in which the subreddits were created. All subreddits in a folder were created in that year. The folder contains one json file for each subreddit (the name of the json file is the name of the subreddit), with the date as key (eg. "20180719") and the value as the list of rules on that date. The list of dates was generated by splitting the content in the rules widget of the subreddit's homepage. Example of how a subreddit's JSON file will look:

```
{

"20180628": [],

"20190729": ["no image posts", "no hate"],

"20190813": ["no image posts", "no hate", "no spam"]

}
```

In the above example on 28th June 2018 (first key), there were no rules. On 29th July 2019, two rules are present. On 13th August 2019, 3 rules are present.


Harita Reddy and Eshwar Chandrasekharan. 2023. Evolution of Rules in Reddit Communities. In Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing (CSCW '23 Companion). Association for Computing Machinery, New York, NY, USA, 278â€“282. https://doi.org/10.1145/3584931.3606973
